# biomedical-continual-pretraining-bert
BERT-based model continually pre-trained using the Masked Language Modeling (MLM) objective on 2 million samples from English PubMed.
